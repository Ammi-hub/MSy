# -*- coding: utf-8 -*-
"""MSyRP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J72AQ52ynCNsJ6sCMwBRzktZI_H4rnfz

⠀⠀

# Predicting Metabolic Syndrome Risk

⠀⠀
⠀⠀
⠀⠀
⠀⠀
⠀⠀

⠀⠀⠀

#1. Data Import

```
1.1 Dataset link - (https://www.kaggle.com/datasets/antimoni/metabolic-syndrome)
```

```
1.2 Import API for the dataset
```
"""

import pandas as pd
import kagglehub
path = kagglehub.dataset_download("antimoni/metabolic-syndrome")
print("Path to dataset files:", path)
df = pd.read_csv(f"{path}/Metabolic Syndrome.csv")
df.rename(columns={'Sex': 'Gender'}, inplace=True)
df = df.drop(columns=['Marital', 'seqn'])
df

"""```

```

# 2. Exploratory Data Exploration (EDA)

```
2.1 Missing Data Visualisation
```
"""

import pandas as pd

print(df.head())
print(df.dtypes)
print(f"Dataset shape: {df.shape}")
print(df.describe())
missing_data = df.isnull().sum()
print("Missing values:\n", missing_data)
import seaborn as sns
import matplotlib.pyplot as plt
print()
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.title("Missing Data Heatmap")
plt.show()

"""```
2.2 Visualising Distribution of numerical data
```
"""

numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

for col in numerical_cols:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], kde=True, bins=20)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

"""```
2.3 Histogram - Categorical Data
```
"""

categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    print(f"\n{col} value counts:")
    print(df[col].value_counts())
    plt.figure(figsize=(8, 4))
    sns.countplot(x=df[col])
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.show()

"""```
2.4 Correlation Matrix
```
"""

correlation_matrix = df[numerical_cols].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)
plt.title("Correlation Matrix")
plt.show()

"""```
2.5 Pairplot - numerical features
```
"""

sns.pairplot(df[['BMI', 'WaistCirc', 'BloodGlucose', 'Income', 'MetabolicSyndrome']], hue='MetabolicSyndrome')
plt.show()

"""```
2.6 Distribution of BMI
```
"""

import seaborn as sns
import matplotlib.pyplot as plt
# Inspect summary statistics for BMI
print("Summary Statistics for BMI:")
print(df["BMI"].describe())

# Visualize BMI distribution
sns.histplot(df["BMI"], kde=True, color="blue")
plt.title("Distribution of BMI")
plt.xlabel("BMI")
plt.ylabel("Frequency")
plt.show()

# Investigate relationships between BMI and Metabolic Syndrome
'''sns.boxplot(x="Metabolic Syndrome", y="BMI", data=df, palette="Set2")
plt.title("BMI vs. Metabolic Syndrome")
plt.xlabel("Metabolic Syndrome")
plt.ylabel("BMI")
plt.show()'''

"""# 3. Pre-processing

```
3.1 Formating / Altering the dataset
```
"""

'''df.rename(columns={'Sex': 'Gender'}, inplace=True)
df = df.drop(columns=['Marital', 'seqn'])
print(df)'''

"""```
3.2 Converting categorical data to numeric
```
"""

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

# Label Encoding to each categorical feature
df['Gender'] = label_encoder.fit_transform(df['Gender'])  # Female=0, Male=1
df['Race'] = label_encoder.fit_transform(df['Race'])  # Convert Race to numerical categories
df

"""```
3.3 Data Cleaning
```
"""

df['Income'] = df['Income'].fillna(df['Income'].mean())
df['BMI'] = df.groupby(['Gender', 'Race'])['BMI'].transform(lambda x: x.fillna(x.mean()))
df['WaistCirc'] = df.groupby(['BMI', 'Gender'])['WaistCirc'].transform(lambda x: x.fillna(x.mean()))
df['WaistCirc'] = df['WaistCirc'].fillna(df['WaistCirc'].mean())
print(df.isnull().sum())

"""# 4. Model Building and Evaluation

```
4.1 Model Assessment
```
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import cross_val_score

X = df.drop(columns=['MetabolicSyndrome'])
y = df['MetabolicSyndrome']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Scale features using StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression with adjusted parameters
log_model = LogisticRegression(max_iter=200, solver='saga')
log_model.fit(X_train_scaled, y_train)

# Predictions
y_pred = log_model.predict(X_test_scaled)

# Evaluate model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Cross-validation
cv_scores = cross_val_score(log_model, X_train_scaled, y_train, cv=5)
print("\nCross-validation Scores:", cv_scores)
print("Average Cross-validation Score:", cv_scores.mean())

"""```
4.2 Model Building and Evaluation
```
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neural_network import MLPClassifier
from sklearn.inspection import permutation_importance
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load and split the dataset
X = df.drop(columns=['MetabolicSyndrome'])
y = df['MetabolicSyndrome']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Logistic Regression
log_reg_model = LogisticRegression(max_iter=200, random_state=42)
log_reg_model.fit(X_train, y_train)
y_pred_lr = log_reg_model.predict(X_test)

# Decision Tree
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

# Random Forest
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Neural Network
nn_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=42)
nn_model.fit(X_train, y_train)
y_pred_nn = nn_model.predict(X_test)

# K-Means Clustering
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(X_scaled)

# DBSCAN Clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Permutation Importance for Random Forest
perm_importance = permutation_importance(rf_model, X_test, y_test, n_repeats=10, random_state=42)

# Model Evaluation
models = {
    "Logistic Regression": (log_reg_model, y_pred_lr),
    "Decision Tree": (dt_model, y_pred_dt),
    "Random Forest": (rf_model, y_pred_rf),
    "Neural Network": (nn_model, y_pred_nn),
}

for model_name, (model, y_pred) in models.items():
    print(f"\n{model_name} Performance:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_ if hasattr(model, 'classes_') else [0, 1])
    disp.plot(cmap="Blues", values_format='d')
    plt.title(f"Confusion Matrix: {model_name}")
    plt.show()

# Visualization of Decision Tree
plt.figure(figsize=(20, 10))
plot_tree(dt_model, feature_names=X.columns, class_names=["0", "1"], filled=True, fontsize=6, proportion=True, max_depth=3)
plt.title("Decision Tree Visualization")
plt.show()

# K-Means Cluster Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# DBSCAN Cluster Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=dbscan_labels, cmap='plasma', alpha=0.7)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Plot Neural Network Loss Curve
plt.figure(figsize=(10, 6))
plt.plot(nn_model.loss_curve_)
plt.title("Neural Network Training Loss Curve")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.grid()
plt.show()

# Permutation Feature Importance Visualization
plt.figure(figsize=(10, 8))
plt.barh(X.columns, perm_importance.importances_mean, color="teal", edgecolor="black")
plt.title("Permutation Feature Importance (Random Forest)")
plt.xlabel("Mean Importance")
plt.ylabel("Features")
plt.show()

"""# 5. Model Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

rf_grid_search = GridSearchCV(estimator=rf_model, param_grid=rf_param_grid, cv=3, n_jobs=-1, verbose=2, scoring='accuracy')
rf_grid_search.fit(X_train, y_train)
print(f"Best Random Forest Parameters: {rf_grid_search.best_params_}")

"""Therefore, In conclusion, Random Forest stands out as the most accurate model for predicting metabolic syndrome in individuals based on the given features. This approach can be useful in healthcare settings to identify individuals at risk of metabolic syndrome early, allowing for timely interventions and management. Further work could include more advanced feature engineering or deep learning techniques to improve performance further."""